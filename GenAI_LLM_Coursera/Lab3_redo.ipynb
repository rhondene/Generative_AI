{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8883454",
   "metadata": {},
   "source": [
    "# Re-do of Lab3 Finetuning FlanT5 with Reinforcement Learning from Human Feedback\n",
    "\n",
    "- üöÄ Motivation is to re-write the code (instead of just re-running cells someone else wrote) from scratch to build muscle memory  üí™üèæüí™üèæüí™üèæ\n",
    "- üß† The goal is to understand the process of fine-tuning LLMs, specifically using Reinforcement Learning from Human Feedback (RLHF) with FlanT5. To this end, this notebook will be heavily annotated with comments as I learn what specific library and lines of code are doing\n",
    "- üìç The ultimate goal is to understand the process of fine-tuning LLMs so that I can apply to mother models like Biological Foundation Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging Face Transformers, Datasets, and PEFT libraries\n",
    "from transformers import(   \n",
    "    pipeline,   # hg high-level API for common tasks using LLMs, kinda like scikit-learn.fit() \n",
    "    AutoTokenizer, # text tokenizer, converts text to numerical tokens for the model\n",
    "   AutoModelForSequenceClassification,  # Facebooks pre-trained BERT model for sentiment classification from text\n",
    "   AutoModelForSeq2SeqLM,  # this sequence-to-sequence model, used fro task like language translation, summarization. This is what Flan-t5 does\n",
    "   GenerationConfig  # used to configure how the model generates text etc\n",
    "   ) \n",
    "\n",
    "from datasets import load_dataset  # hg high-level API for downloading and reproducibily managing ML datasets, kunda like scikit-learn.load_dataset()\n",
    "#library for Parameter-Efficient Fine-tuning LLMs \n",
    "from peft import (\n",
    "     PeftModel,   #  wrappers for efficient fine-tuning of LLM, recall that peft updates only small fraction of the model params\n",
    "     PeftConfig,  \n",
    "     LoraConfig,  # Low-rank Adaptation, another peft method, which fine-tunes small number of adapter layers instead of the whole model saves lots of memory\n",
    "     TaskType  # used to specify the type of task I am fine-tunining for \n",
    " )\n",
    "#trl: Transformers Reinforcement Learning library \n",
    "from trl import( PPOTrainer,  #implement the proximal policy optimization algo\n",
    "                PPOConfig, \n",
    "                AutoModelForSeq2SeqLMWithValueHead, #a value head in RLHF is a neural network layer added to the LLm for estimating how good the model's output is\n",
    "                create_reference_model,\n",
    "\n",
    ")\n",
    "from trl.core import LengthSampler # samples sequences from the dataset based on length criteria\n",
    "\n",
    "import torch \n",
    "import evaluate \n",
    "import numpy  as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa82f3e",
   "metadata": {},
   "source": [
    "# Load FLAN-T5 Model, Prepare RewardModel and Toxicity Evaluator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba64e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12460/12460 [00:00<00:00, 110431.70 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 73314.18 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [00:00<00:00, 125063.73 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "huggingface_dataset_name='knkarthick/dialogsum'\n",
    "dataset_original = load_dataset(path=huggingface_dataset_name) #path can be the HuggingFace dataset ID or a local path \n",
    "dataset_original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b059871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'dialogue', 'summary', 'topic']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_original['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6fd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...</td>\n",
       "      <td>Mr. Smith's getting a check-up, and Doctor Haw...</td>\n",
       "      <td>get a check-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>#Person1#: Hello Mrs. Parker, how have you bee...</td>\n",
       "      <td>Mrs Parker takes Ricky for his vaccines. Dr. P...</td>\n",
       "      <td>vaccines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>#Person1#: Excuse me, did you see a set of key...</td>\n",
       "      <td>#Person1#'s looking for a set of keys and asks...</td>\n",
       "      <td>find keys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>#Person1#: Why didn't you tell me you had a gi...</td>\n",
       "      <td>#Person1#'s angry because #Person2# didn't tel...</td>\n",
       "      <td>have a girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>#Person1#: Watsup, ladies! Y'll looking'fine t...</td>\n",
       "      <td>Malik invites Nikki to dance. Nikki agrees if ...</td>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           dialogue  \\\n",
       "0  train_0  #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. ...   \n",
       "1  train_1  #Person1#: Hello Mrs. Parker, how have you bee...   \n",
       "2  train_2  #Person1#: Excuse me, did you see a set of key...   \n",
       "3  train_3  #Person1#: Why didn't you tell me you had a gi...   \n",
       "4  train_4  #Person1#: Watsup, ladies! Y'll looking'fine t...   \n",
       "\n",
       "                                             summary              topic  \n",
       "0  Mr. Smith's getting a check-up, and Doctor Haw...     get a check-up  \n",
       "1  Mrs Parker takes Ricky for his vaccines. Dr. P...           vaccines  \n",
       "2  #Person1#'s looking for a set of keys and asks...          find keys  \n",
       "3  #Person1#'s angry because #Person2# didn't tel...  have a girlfriend  \n",
       "4  Malik invites Nikki to dance. Nikki agrees if ...              dance  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect first 5 rows in the train split\n",
    "dataset_original['train'].to_pandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcc4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12460/12460 [00:00<00:00, 227484.95 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10026/10026 [00:03<00:00, 3120.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 9023\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 1003\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#write function for preprocessing the dataset\n",
    "\n",
    "def build_dataset(model_name, \n",
    "                  dataset_name,\n",
    "                  input_min_text_length,\n",
    "                  input_max_text_length,):\n",
    "    \n",
    "    #------Load the daset from training\n",
    "    dataset = load_dataset(dataset_name, split='train')\n",
    "    #filter the dialogues based on text limits\n",
    "    dataset = dataset.filter(lambda x: \n",
    "                             len(x['dialogue']) >= input_min_text_length and len(x['dialogue']) <= input_max_text_length,batched=False\n",
    "\n",
    "                             )\n",
    "    #------Tokenization \n",
    "    #instantiate the tokenizer for the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name,device_map='auto')\n",
    "    def tokenize(sample):\n",
    "        #create instruction prompt template for each piece of dialogue\n",
    "        #the prompt template is a string that will be used to instruct the model to summarize the dialogue\n",
    "        prompt = f\"\"\" Summarize the following conversation \n",
    "                      {sample['dialogue']}\n",
    "                        Summary:\n",
    "                \"\"\"\n",
    "        sample['input_ids'] = tokenizer.encode(prompt) # encodes text as unique vector integer e.g 'how may i help you' -> [101, 2129, 2026, 1010, 2129, 2017, 102]\n",
    "        # for RLFH, we need to input ids as query because the the PPO library needs to access the human-readbale text \n",
    "        #the model will read the input ids and generate token ids, that the RLHF can then use to score\n",
    "        sample['query'] = tokenizer.decode(sample['input_ids']) #has to be called 'query' for the PPO library\n",
    "        \n",
    "        return sample\n",
    "    #tokenize each dialogue\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type='torch')\n",
    "    #Split the dataset into train and test parts\n",
    "    dataset_splits =  dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    return dataset_splits\n",
    "\n",
    "    \n",
    "#call the function to build the dataset\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=huggingface_dataset_name,\n",
    "                        input_min_text_length=20,  #minimum length of the dialogue\n",
    "                        input_max_text_length=1000,  #maximum length of the dialogue\n",
    "                        )\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5bac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_11040</td>\n",
       "      <td>#Person1#: Good afternoon, Mr. Chen. How are y...</td>\n",
       "      <td>#Person1# helps Mr. Chen change some Hong Kong...</td>\n",
       "      <td>currency exchange</td>\n",
       "      <td>[12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...</td>\n",
       "      <td>Summarize the following conversation #Person1#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_5748</td>\n",
       "      <td>#Person1#: Are you new here?\\n#Person2#: Yes. ...</td>\n",
       "      <td>Monica helps Wilson fax an order to the head o...</td>\n",
       "      <td>office talk</td>\n",
       "      <td>[12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...</td>\n",
       "      <td>Summarize the following conversation #Person1#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_12033</td>\n",
       "      <td>#Person1#: I want to go house shopping, and I ...</td>\n",
       "      <td>#Person2# helps #Person1# calculate the price ...</td>\n",
       "      <td>House affordability evaluation</td>\n",
       "      <td>[12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...</td>\n",
       "      <td>Summarize the following conversation #Person1#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_8325</td>\n",
       "      <td>#Person1#: I'd like to buy one of these refrig...</td>\n",
       "      <td>#Person1# wants to buy a refrigerator. #Person...</td>\n",
       "      <td>easy-payment plan</td>\n",
       "      <td>[12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...</td>\n",
       "      <td>Summarize the following conversation #Person1#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_6503</td>\n",
       "      <td>#Person1#: I am having some plumbing problems ...</td>\n",
       "      <td>#Person1# is having some plumbing problems in ...</td>\n",
       "      <td>plumbing problem</td>\n",
       "      <td>[12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...</td>\n",
       "      <td>Summarize the following conversation #Person1#...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           dialogue  \\\n",
       "0  train_11040  #Person1#: Good afternoon, Mr. Chen. How are y...   \n",
       "1   train_5748  #Person1#: Are you new here?\\n#Person2#: Yes. ...   \n",
       "2  train_12033  #Person1#: I want to go house shopping, and I ...   \n",
       "3   train_8325  #Person1#: I'd like to buy one of these refrig...   \n",
       "4   train_6503  #Person1#: I am having some plumbing problems ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  #Person1# helps Mr. Chen change some Hong Kong...   \n",
       "1  Monica helps Wilson fax an order to the head o...   \n",
       "2  #Person2# helps #Person1# calculate the price ...   \n",
       "3  #Person1# wants to buy a refrigerator. #Person...   \n",
       "4  #Person1# is having some plumbing problems in ...   \n",
       "\n",
       "                            topic  \\\n",
       "0               currency exchange   \n",
       "1                     office talk   \n",
       "2  House affordability evaluation   \n",
       "3               easy-payment plan   \n",
       "4                plumbing problem   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...   \n",
       "1  [12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...   \n",
       "2  [12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...   \n",
       "3  [12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...   \n",
       "4  [12198, 1635, 1737, 8, 826, 3634, 1713, 345, 1...   \n",
       "\n",
       "                                               query  \n",
       "0  Summarize the following conversation #Person1#...  \n",
       "1  Summarize the following conversation #Person1#...  \n",
       "2  Summarize the following conversation #Person1#...  \n",
       "3  Summarize the following conversation #Person1#...  \n",
       "4  Summarize the following conversation #Person1#...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect the processed dataset\n",
    "dataset['train'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79d7d8",
   "metadata": {},
   "source": [
    "# Load and Prepare the PEFT model for RLHF\n",
    "\n",
    "- This steps sets up the LoRA for efficient fine-tuning. Instead of updating all 247M parameters of the FLAN-T5,\n",
    "- we add a small adapter layers that will be trained, while the rest of the model remains frozen.\n",
    "- `Input`: Raw Flan-T5 base model + Pre-trained LoRA adapters (downloaded from S3)\n",
    "- `Output`: A hybrid peft_model ready for RLHF training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97065956",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the model trained from lab2\n",
    "import os \n",
    "import boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39be3b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: models/peft-dialogue-summary-checkpoint/adapter_config.json\n",
      "Downloaded: models/peft-dialogue-summary-checkpoint/adapter_model.bin\n",
      "Downloaded: models/peft-dialogue-summary-checkpoint/special_tokens_map.json\n",
      "Downloaded: models/peft-dialogue-summary-checkpoint/tokenizer.json\n",
      "Downloaded: models/peft-dialogue-summary-checkpoint/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3') #set s3 client\n",
    "bucket_name= 'dlai-generative-ai' #name of the bucket\n",
    "s3_prefix= 'models/peft-dialogue-summary-checkpoint/'\n",
    "local_dir='./peft-dialogue-summary-checkpoint-from-s3/' #to save the model\n",
    "\n",
    "os.makedirs(local_dir, exist_ok=True) \n",
    "\n",
    "#download the model from S3 bucket\n",
    "paginator= s3.get_paginator('list_objects_v2')\n",
    "for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
    "    if 'Contents' in page:\n",
    "        for obj in page['Contents']:\n",
    "            key = obj['Key']\n",
    "            local_file = os.path.join(local_dir, key.replace(s3_prefix, ''))\n",
    "            local_file_dir = os.path.dirname(local_file)\n",
    "            os.makedirs(local_file_dir, exist_ok=True)\n",
    "            s3.download_file(bucket_name, key, local_file)\n",
    "            print(f\"Downloaded: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d24b9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to help us track the number of trainable and all model parameters to ensure PEFT is working as expected\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model) :\n",
    "    \"\"\"Prints the number of trainable and all model parameters.\n",
    "    Args:\n",
    "        model: a PEFT model object.  This model is PefModel object thats a wrapper around the original model, in this case Flan-T5.\n",
    "    \"\"\"\n",
    "    trainable_model_params=0\n",
    "    all_model_params=0\n",
    "    for _, params in model.named_parameters(): #iterates over the parameters in each layer of the model\n",
    "        all_model_params += params.numel()\n",
    "        if params.requires_grad:\n",
    "            trainable_model_params += params.numel()\n",
    "    print(f\"Trainable model parameters: {trainable_model_params}\")\n",
    "    print(f\"All model parameters: {all_model_params}\")\n",
    "    print(f\"Percentage of trainable model parameters: {trainable_model_params/all_model_params*100:.2f}%\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7186e0f8",
   "metadata": {},
   "source": [
    "#### Add the peft adapter to the original Flan-T5 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b490dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#configure the LoRA adapter and how it will be applied to the model\n",
    "lora_config = LoraConfig(\n",
    "    r= 32,  #controls rank for decomposition, higher rank means more parameters to train, so more accurate but also higher compute cost \n",
    "    lora_alpha=32, \n",
    "    target_modules=['q','v'], # add adpaters to the query and value layers of the attention mechanism in the transformer model\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",  #no bias in the LoRA layers,\n",
    "    task_type= TaskType.SEQ_2_SEQ_LM  #Flan-T5 is a sequence-to-sequence model \n",
    ")\n",
    "#load the base model \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype= torch.float16,  #use float16 for faster training and less memory usage\n",
    "    device_map='cpu',  #use all available GPUs\n",
    ")\n",
    "\n",
    "#here we load the pre-trained LoRA adapter layers from S3, and then wrap the base model with the PEFT model.\n",
    "#so the `peft_model` is hybrid model that combines the frozen weights of the Flan-T5 base model with the trainable LoRA adapters\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id = local_dir,  #path to the pre-trained LoRA adapter layers downloaded from S3\n",
    "    lora_config = lora_config,  #LoRA config\n",
    "    torch_dtype=torch.float16,  \n",
    "    device_map='cpu',  #use all available GPUs,\n",
    "    is_trainable=True  #set the model to trainable mode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6014d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable model parameters: 3538944\n",
      "All model parameters: 251116800\n",
      "Percentage of trainable model parameters: 1.41%\n",
      "PEFT model parameters to be updated: None\n"
     ]
    }
   ],
   "source": [
    "print(\"PEFT model parameters to be updated:\", print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09f62563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable model parameters: 3539713\n",
      "All model parameters: 251117569\n",
      "Percentage of trainable model parameters: 1.41%\n",
      "PPO model parameters to be updated (ValueHead + 769 params): None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ValueHead(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    peft_model,\n",
    "    torch_dtype=torch.float16,  \n",
    "    is_trainable=True,  #set the model to trainable mode\n",
    "    device_map='cpu',  #use all available GPUs\n",
    ")\n",
    "#see how many trainable paramters the pp model has \n",
    "print(\"PPO model parameters to be updated (ValueHead + 769 params):\", print_number_of_trainable_model_parameters(ppo_model))\n",
    "ppo_model.v_head\n",
    "##notic how the number of trainable params for ppo 3,539,713 is slightly higher than the peft model 3,538,944,\n",
    "#this is because of dimension of the valuehead the PPO model (768)  +bias (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e8b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = create_reference_model(ppo_model)  \n",
    "# this is copy of the PPO model before training, used for KL divergence to compare the newly trained PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4bbd9",
   "metadata": {},
   "source": [
    "# Prepare the Reward Model \n",
    "\n",
    "- Reinforcement learning is a machine learning paradigm in which agents take action in an environment aimed at maximimized their cumulative rewards. The agent's behavior is deinied by the `policy`.\n",
    "- The goal of the reinforcement learning is for the agent is learn the optimal policy that maximises the reard function.\n",
    "- In the previous section, the original policy is based on the PEFT model - .e. the LLM before detoxification.\n",
    "- Now we will define the reward model encouraging the agent to detoxify the dialogue summaries. The intutive approach is to do sentiment analysis across the two classes (`nothate` and `hate`) and give higher reward if there is chance of gettuing `nothate` class.\n",
    "\n",
    "# Load the reward model\n",
    "- The reward model is `Facebooks RoBERTa based on hate-speech classification model`. \n",
    "- This model outputs  `logits` and then predict probabilities across the two claseses (`nothate` and `hate`). \n",
    "- The logits of the output will be tabken as a positive reward. \n",
    "- Then the model will be fine-tuned with `PPO` using those reward values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa1944e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name= 'facebook/roberta-hate-speech-dynabench-r4-target'\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=toxicity_model_name, device_map='auto')\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    toxicity_model_name,\n",
    "    device_map='cpu',\n",
    ")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea185149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Non-toxic Logits [not hate, hate]: [4.645077228546143, -4.235363006591797]\n",
      " Non-toxic Probabilities [not hate, hate]: [0.9998608827590942, 0.00013906363165006042]\n",
      " Toxic Logits [not hate, hate]: [-1.392095685005188, 0.9838298559188843]\n",
      " Toxic Probabilities [not hate, hate]: [0.08502701669931412, 0.9149730205535889]\n"
     ]
    }
   ],
   "source": [
    "#test the binary classifier for toxicity sentiment analysis\n",
    "\n",
    "non_toxic_text='I love you so much, you are the best!'\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors='pt').input_ids #input_ids is the integer vector of the text in the form of a pytorch tensor\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits #forward-pass the tokenized text to get the logit predictions\n",
    "probabilities = logits.softmax(dim=1).tolist()[0]   #softmax maps logits (real nums) to probability scale (0-1)\n",
    "print(f\" Non-toxic Logits [not hate, hate]: {logits.tolist()[0]}\")\n",
    "print(f\" Non-toxic Probabilities [not hate, hate]: {probabilities}\")\n",
    "\n",
    "toxic_text='i want to kill you'\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors='pt').input_ids\n",
    "logits = toxicity_model(input_ids = toxicity_input_ids).logits\n",
    "probabilities = logits.softmax(dim=1).tolist()[0]\n",
    "print(f\" Toxic Logits [not hate, hate]: {logits.tolist()[0]}\")\n",
    "print(f\" Toxic Probabilities [not hate, hate]: {probabilities}\")\n",
    "# the \"not hate\" predicted probablities of the model becomes the Reward Score for the PPO model to train it to avoiid hate speech \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ea95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use hugginhface inference pipeline\n",
    "device=0 if torch.cuda_is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment_analysis\",\n",
    "                          model= toxicity_model_name,\n",
    "                          device='cpu')\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\":None, #return all scores\n",
    "    \"function_to_apply\": \"none\", \n",
    "    \"batch_size\":16}\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\":None,\n",
    "    \"function_to_apply\": \"softmax\",\n",
    "    \"batch_size\":16}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
    "                                   toxicity_model_name,\n",
    "                                   module_type='measurement',\n",
    "                                   toxic_label='hate')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI_practive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
