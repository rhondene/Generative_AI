{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e2b775f",
   "metadata": {},
   "source": [
    "\n",
    "In this lesson, you will familiarize yourself with the chatbot example you will work on during this course. The example includes the tool definitions and execution, as well as the chatbot code. Make sure to interact with the chatbot at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27134ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /Users/rwint/miniconda3/envs/genai_ml/lib/python3.11/site-packages (from arxiv) (2.32.5)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /Users/rwint/miniconda3/envs/genai_ml/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rwint/miniconda3/envs/genai_ml/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rwint/miniconda3/envs/genai_ml/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rwint/miniconda3/envs/genai_ml/lib/python3.11/site-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/rwint/miniconda3/envs/genai_ml/lib/python3.11/site-packages (from dotenv) (1.1.1)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "\u001b[33m  DEPRECATION: Building 'sgmllib3k' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sgmllib3k'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=ed5506e5972436ec33b823abe5452cfae552885f5ccd2dc96bc3e91f67c1c6f8\n",
      "  Stored in directory: /Users/rwint/Library/Caches/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, dotenv, arxiv\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [arxiv]\n",
      "\u001b[1A\u001b[2KSuccessfully installed arxiv-2.2.0 dotenv-0.9.9 feedparser-6.0.12 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68321652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import arxiv\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import json \n",
    "import langextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd14298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-an\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # This loads the .env file\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    ")\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')  # Now you can access the variables\n",
    "print(api_key[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afcbb3",
   "metadata": {},
   "source": [
    "# Tool Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cee0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_DIR='papers'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d5bd8",
   "metadata": {},
   "source": [
    "- this tool searches arXiv papers based on a topic and stores the papers info in a JSOn (title, author, paper url, punlication date).\n",
    "- The JSON files are organized in the `papers` directory.\n",
    " - The tool does NOT download the papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0acd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 papers to papers/rna/papers.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2411.08900v1', '1512.06979v1', '1405.3390v2', '2303.14065v1', '2502.00647v1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_papers(topic:str, max_results:int=5):\n",
    "    \"\"\"Search for papers on arXiv based on atopic and store their infornmation \n",
    "    Args:\n",
    "        topic (str): The topic to search for.\n",
    "        max_results (int, optional): Maximum number of results to return. Defaults to 5.\n",
    "    \"\"\"\n",
    "    #use arxiv to search papers based on the topic\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(query=topic, \n",
    "                          max_results=max_results, \n",
    "                          sort_by=arxiv.SortCriterion.Relevance)\n",
    "    papers = client.results(search)\n",
    "\n",
    "    #storage folder for papers named by the topic\n",
    "    path=  os.path.join(PAPER_DIR, topic.lower().replace(' ', '_') )\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    file_path= os.path.join(path, 'papers.json')\n",
    "    #load existing papers if any \n",
    "    try:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            papers_info=json.load(json_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        papers_info={}\n",
    "    \n",
    "    #process each queried paper\n",
    "    paper_ids = []\n",
    "    for paper in papers:\n",
    "        paper_ids.append(paper.get_short_id() )\n",
    "        paper_info = {\n",
    "            'title': paper.title,\n",
    "            'authors': [author.name for author in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'pdf_url': paper.pdf_url,\n",
    "            'published': str(paper.published.date()),\n",
    "\n",
    "        }\n",
    "        papers_info[paper.get_short_id()] = paper_info\n",
    "\n",
    "    #save update papers infor\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(papers_info, json_file, indent=4)\n",
    "    print(f\"Saved {len(paper_ids)} papers to {file_path}\")\n",
    "    return paper_ids\n",
    "\n",
    "\n",
    "search_papers(\"RNA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c133f7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"RNA-GPT: Multimodal Generative System for RNA Sequence Understanding\",\n",
      "  \"authors\": [\n",
      "    \"Yijia Xiao\",\n",
      "    \"Edward Sun\",\n",
      "    \"Yiqiao Jin\",\n",
      "    \"Wei Wang\"\n",
      "  ],\n",
      "  \"summary\": \"RNAs are essential molecules that carry genetic information vital for life,\\nwith profound implications for drug development and biotechnology. Despite this\\nimportance, RNA research is often hindered by the vast literature available on\\nthe topic. To streamline this process, we introduce RNA-GPT, a multi-modal RNA\\nchat model designed to simplify RNA discovery by leveraging extensive RNA\\nliterature. RNA-GPT integrates RNA sequence encoders with linear projection\\nlayers and state-of-the-art large language models (LLMs) for precise\\nrepresentation alignment, enabling it to process user-uploaded RNA sequences\\nand deliver concise, accurate responses. Built on a scalable training pipeline,\\nRNA-GPT utilizes RNA-QA, an automated system that gathers RNA annotations from\\nRNACentral using a divide-and-conquer approach with GPT-4o and latent Dirichlet\\nallocation (LDA) to efficiently handle large datasets and generate\\ninstruction-tuning samples. Our experiments indicate that RNA-GPT effectively\\naddresses complex RNA queries, thereby facilitating RNA research. Additionally,\\nwe present RNA-QA, a dataset of 407,616 RNA samples for modality alignment and\\ninstruction tuning, further advancing the potential of RNA research tools.\",\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2411.08900v1\",\n",
      "  \"published\": \"2024-10-29\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_info(paper_id:str)->json:\n",
    "    \"\"\"Extract information from a paper given its ID.\n",
    "    Returns a JSON object with the paper's title, authors, summary, and PDF URL.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for item in os.listdir(PAPER_DIR):\n",
    "        item_path = os.path.join(PAPER_DIR, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            file_path=os.path.join(item_path, 'papers.json')\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with open(file_path, \"r\") as json_file:\n",
    "                        papers_info = json.load(json_file)\n",
    "                        if paper_id in papers_info:\n",
    "                            return json.dumps(papers_info[paper_id], indent=2)\n",
    "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "                    continue\n",
    "    return f\"There's no saved information related to paper ID {paper_id}.\"\n",
    "\n",
    "print(extract_info('2411.08900v1'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b1c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
